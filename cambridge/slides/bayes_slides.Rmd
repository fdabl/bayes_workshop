---
title: "Bayesian statistics"
subtitle: "Fresh Statistics in Psychology | JRP Conference 2015"
author: |
  | Peter A. Edelsbrunner
  | ETH Zurich, Learning Sciences
  | &
  | Fabian Dablander
  | University of Tubingen, Cognitive Science
bibliography: "../../bibliography.bib"
csl: "../../apa.csl"
output:
  ioslides_presentation:
    widescreen: yes
    incremental: false
    css: styles.css
---

<!--
10 min [10] Peter: A few remarks on the "replication crisis" in psychology
10 min [20] Peter: Statistics: The Status Quo
15 min [35] Peter: Problems with current statistical practices
20 min [55] Fabian: The principled Bayesian
15 min [70] Fabian: The practical Bayesian
40 min [110] Peter: Software for Bayesian data analysis
10 min [120] Peter: Discussion
-->

## Note
- please install http://jasp-stats.org
- and Mplus if you like (on the stick)
- if you're a hacker, git clone the repo **dostodabsi/bayes_workshop**
- otherwise wait for the USB-stick; all materials are on there

# Statistics

## Statistics

> - There is no truth

> - No correct way

> - Everyone does different things

## Statistics

> - So what?

<div align="center">
<img height='500' src='img/bull.png'>
</div>

## Statistics

Silberzahn, R., et al. (2015). Crowdsourcing data analysis: Do soccer referees give more red cards to dark skin toned players? Center for Open Science, https://osf.io/j5v8f/

> - 29 teams of statisticans around the world

> - Same data set, same question

> - What do they do?

## Statistics

<div align="center">
<img height='500' src='img/rc2.png'>
</div>

## Statistics

- What would you do?

<div align="center">
<img height='500' src='img/data.png'>
</div>

## Statistics

- What do they do?

<div align="center">
<img height='500' src='img/models.png'>
</div>

## Statistics

- What do they find?

<div align="center">
<img height='500' src='img/odds.png'>
</div>

## Statistics

> - So what?

- This is part of a crisis in Psychology

> - Statistics **can** be useful

# Bayesian Statistics
  
## Outline

- 0 A few remarks on the "replication crisis" in psychology
<!-- Peter, 10 min [10] -->

- **Statistics: The Status Quo** <!-- Peter, 10 min [20] -->
    * 1.1 An Example
    * 1.2 Notes on the history of statistics

- **Problems with current statistical practices** <!-- Peter, 15 min [35] -->
    * 2.1 p-Value Hypothesis Testing
    * 2.2 Confidence intervals: A Solution?

## Outline

- **The principled Bayesian** <!-- Fabian, 20 min [55] -->
    * 3.1 Probability and Bayes' rule
    * 3.2 Binomial example (coin tosses -- what else?)
    * 3.4 Model comparison using Bayes factor

- **The practical Bayesian** <!-- Fabian, 15 min [70] -->
    * 4.1 Creativity example
    * 4.2 Monte Carlo methods
    * 4.3 Markov chain Monte Carlo methods
    * 4.4 Model adequacy and posterior predictive checks

## Outline

- **Software for Bayesian data analysis** <!-- Peter, 40 min [110] -->
    * 5.1 JASP: Examples
    * 5.2 JASP: Practice
    * 5.2 Demo: Bayesian SEM

- **Discussion** <!-- Peter, 10 min [120] -->

    * Outlook
    * Questions

# Statistics: The Status Quo <!-- Peter -->

## An Example

DATASET FIGURE RESULTS
P-VALUE INTERPRETATION WITH AUDIENCE

## History

FISHER ETC.


## put succinctly ...
> "The textbooks are wrong. The teaching is wrong. The seminar
> you just attended is wrong. The most prestigious journal in your
> scientific field is wrong.

> - Ziliak and McCloskey (2008)

## An Example



## An Example: Creativity and Intelligence <!-- Peter -->

```{r, eval=FALSE}
creadata <- read.csv("creadata_bayes.csv") # Load data
summary(lm(flue ~ int + int_sq, data = creadata)) # Multiple regression
```

<div align="center">
<img height="300" src="img/Rlinreg.png">
</div>

## An Example: Risk Behaviour and Social Status <!-- Peter -->

- What does *p* tell us?

- *The probability to get data this or more extreme in case that in the underlying population there is no association between popularity/likeability and risk behaviour is less than 0.1%*

- Does this sound *strange* to you?


## history: how did we get here? <!-- Fabian -->
- quick question: how old do you think is **modern statistics**?

## Ronald A. Fisher
<img src="img/fisher.jpg" width=500 height=500 />


## Ronald A. Fisher
- **Statistical Methods for Research Workers** (1925)
- **Design of Experiments** (1935)
- only talked about $H_0$ and $\alpha$; no alternative hypothesis! no power!
- p-value indicates **strength of evidence**!
- that is, $p = 0.001$ is **better** than $p = 0.049$


## Ronald A. Fisher
> "No scientific worker has a fixed level of significance at which from year to year, and in all
> circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light
> of his evidence and his ideas."

- R.A. Fisher (1956), as cited in [@gigerenzer2014surrogate, p.1]

## Jerzy Neyman and Egon Pearson
- extended Fisher's work, but **changed it substantially**
- introduced the alternative hypothesis, $H_1$
- concept of a $\beta$ error; statistical power
- $p < \alpha$, or $p > \alpha$
- binary cut-offs; not **statistical evidence**!
- they did **behavioral statistics**

## Neyman-Pearson
> "We are inclined to think that as far
> as a particular hypothesis is concerned, no test based upon the theory
> of probability can by itself provide any valuable evidence of
> the truth or falsehood of that hypothesis.


## Neyman-Pearson
> "We are inclined to think that as far
> as a particular hypothesis is concerned, no test based upon the theory
> of probability can by itself provide any valuable evidence of
> the truth or falsehood of that hypothesis.

> But we may look at the
> purpose of tests from another view-point. **Without hoping to know**
> **whether each separate hypothesis is true or false**, we may search
> for rules to **govern our behaviour** with regard to them, in following
> which we insure that, in the long run of experience, we shall not be too often wrong."

- Neyman and Pearson (1933), as cited in [@johansson2011hail, p.118]

## unification / bastardization
- in the 40s and 50s **psychologists** (i.e. Guilford) wrote popular statistics books
- they **did not** distinguish between Fisher's and Neyman-Pearson's approach
- current statistical practice is a hybrid between those two **incompatible** paradigms

## unification / bastardization
> statisticians "have already overrun every branch of science with a rapidity of conquest
> rivalled only by Attila, Mohammed, and the Colorado beetle."

- did Piaget ever compute a p-value?
- did Skinner? what's with Köhler? Pavlov?

# Problems with Current Statistical Practices

## p-Value Hypothesis Testing

NHST BASHING

## mindless statistics
- 1a: setup a statistical hypothesis of **no difference** or **zero correlation**
- 1b: don't specify the predictions of your research hypothesis
- 2: Use 0.05 as a convention for rejecting the null
- 3: always use this procedure [@gigerenzer2004mindless]


## what is probability? <!-- Fabian -->
- for frequentists, probability is the long-run relative frequency of events
- for example, the probability of a coin coming up heads is the proportion of heads in an **infinite** amount of tosses
- to ask about the probability of the next coin toss being heads is **nonsensical**
- the next coin toss is either heads, or it is not
- single events can't be assigned probability!
- **repeatability** becomes a crucial ingredient
- **Fisher**: geneticist, worked in agriculture, did lots of repeatable experiments
- **Jeffreys**: geophysicist, studied earthquakes etc., no way of repeating experiments

## what is probability?
- frequentists cannot talk about
 * the probability of a 3rd world war
 * climate change
 * you failing your next exam
 * any non repeatable event
- these issues are about **uncertainty**, and need a Bayesian take on probability
- by conceptualising probability as long-run average frequency, classical statistics deprived itself of answering the essential question in science
- ***what is the probability that my hypothesis / theory is true?***

# what is in a p-value? <!-- Fabian -->

## some statements
- suppose you run an experiment testing the effect of some treatment
- you have a control group (n = 20) and a treatment group (n = 20)
- you run an independent t-test and get $t(18) = 2.7, p = 0.01$
- let's rate some statements!
- from @haller2002misinterpretations, who got it from @oakes1986statistical

## statement 1
- You have absolutely disproved the null hypothesis of no difference.

## statement 1
<img src="img/hell_NO.jpg" width=400 height=400 />

## statement 1
- You have absolutely disproved the null hypothesis of no difference.
- everything in statistics is probabilistic, there are no **absolutes**

## statement 2
- You have found the probability of the null hypothesis being true.

## statement 2
<img src="img/cat_NO.jpg" width=400 height=400 />

## statement 2
- You have found the probability of the null hypothesis being true.
- in classical statistics, you cannot assign probabilities to your hypotheses
- but every researcher really, really, really, really wants that!
- one reason for Bayesian statistics ;)

## statement 3
- You have absolutely proved your experimental hypothesis (there is a difference).

## statement 3
<img src="img/hell_NO.jpg" width=400 height=400 />

## statement 3
- You have absolutely proved your experimental hypothesis (there is a difference).
- same as statement 1: probabilistic, not absolute!

## statement 4
- You can deduce the probability of the experimental hypothesis being true.

## statement 4
<img src="img/cat_NO.jpg" width=400 height=400 />

## statement 4
- You can deduce the probability of the experimental hypothesis being true.
- same as statement 2: can't assign probabilities to hypotheses!

## statement 5
- You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.

## statement 5
<img src="img/classic_NO.jpg" width=400 height=400 />

## statement 5
- You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision.
- ummm, isn't that $\alpha$?
- **classic** NO: $\alpha$ is about an infinite amount of experiments
- in this case, the probability is $1 - p(H_0)$
- cannot get $p(H_0)$ without Bayes, so this statement is wrong


## statement 6
- You have a reliable experimental finding in the sense that, if the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.

## statement 6
<img src="img/unsure_NO.jpg" width=400 height=400 />

## statement 6
- You have a reliable experimental finding in the sense that, if the experiment were repeated a great number of times, you would obtain a significant result on 99% of occasions.
- that's the **replication fallacy** [@haller2002misinterpretations]

## how often do people get it wrong?
<img src="img/haller_misinterpretation.png" width=600 height=500 />

# frequentist inference <!-- Fabian -->

## how a frequentist draws inference
- suppose you are being asked 26 questions about Bayesian statistics ($N = 26$)
- you happen to answer 8 correctly ($k = 8$)
- are you better than guessing?
- test $H_0: \theta = \frac{1}{2}$ against $H_1: \theta \neq \frac{1}{2}$
  
## how a frequentist draws inference
- to obtain the **sampling distribution** assume that $H_0: \theta = \frac{1}{2}$ holds
- imagine that the experiment is repeated very many times under **identical circumstances**
- for each **hypothetical** experiment, calculate the test statistic of the **hypothetical** sample
- the distribution of these test statistics are called the **sampling distribution**

## graphically [@wagenmakers2007practical]
<img src="img/sampling_distribution.png" />

## sampling plan
- there are two different ways to conduct the experiment
- either fix $N = 26$ and count the number of correct answers
- or ask so many questions until one gets $k = 8$ correct answers
- first one is binomial, the second one is negative binomial
- $p(k | \theta, N) = \binom{n}{k} \theta^k (1 - \theta)^{n - k}$
- $p(N | \theta, z) = \binom{n - 1}{k - 1} \theta^k (1 - \theta)^{n - k}$

## note
- a frequentist **needs** this sampling distribution
- often, properties of the sampling distribution can be derived analytically from the sample data
- for example, the variance of the **sampling distribution** of the **sample mean** is $\sigma^2 = \frac{s^2}{N}$
- this is one reason why frequentism got so much traction: it is computationally trivial

## bootstrapping
- in order to **see** what happens here, let us simulate a large number of replications of the experiment!
- we will draw independent random samples from our actual observed data, and compute the test statistic of that draw
- in the binomial case, the test statistic is the **number of correct answers**
- in the negative binomial case, the test statistic is the **number of questions**

## binomial bootstrapping
```{r}
binom.boot <- function(dat, samples = 10000) {
    n <- length(dat) # is fixed!
    result <- numeric(samples)

    for (i in 1:samples) {
        draw <- sample(dat, n, replace = TRUE)
        result[i] <- sum(draw)
    }

    result
}
```

## negative binomial bootstrapping
```{r}
nbinom.boot <- function(dat, samples = 10000) {
    k <- sum(dat) # is fixed!
    result <- numeric(samples)

    for (i in 1:samples) {
        ss <- sample(c(0, 1), 1, prob = c(0.5, 0.5))

        while (sum(ss) != k) {
            ss <- c(ss, sample(c(0, 1), 1, prob = c(0.5, 0.5)))
        }

        result[i] <- length(ss)
    }
    result
}
```

## practical difference
<img src="img/sampling_binomial.png" height="500" width="700"/>

## frequentist inference
- assumes that the parameter $\theta$ is fixed
- only the data is allowed to vary
- we can intuit that confidence intervals, statistical power etc. are not properties of the data
- they are properties of the **testing procedure**


# p-values: a death in five acts <!-- Fabian -->

## act I <!-- Fabian -->
<img src="img/sampling_distribution.png" width=700 height=500 />


## act I <!-- Fabian -->
- result of statistical analysis depends on the intention of the researcher
- because those intentions define the space of all possible (unobserved) data [@wagenmakers2007practical]


## act I
- suppose I ask you 26 questions on Bayesian stats, and you get 8 right
- the last question you answered, you answered correctly
- were you better than chance? $H_0: \theta = 0.5$
- there are two possible sampling plans
- ask 26 questions, and see how many you answered correctly (**binomial**)
- ask questions so long until you answer 8 correctly (**negative binomial**)
- ***the data are the data are the data*** (example from Kruschke, 2010, Ch. 11)

## act I
<img src="img/sampling_binomial.png" width=700 height=500 />

## act I
```{r}
N <- 26
k <- 8

binomial <- pbinom(k, N, 0.5) * 2
negbinomial <- pnbinom(k, N, 0.5) * 2

binomial # fail to reject

negbinomial # science paper!!
```


## act II <!-- Fabian -->
- optional stopping [@sanborn2014frequentist; @rouder2014optional]
- look at data, test, gather more data, test, etc.

## act II, demo
```{r, message = FALSE, eval = FALSE}
source("http://rynesherman.com/phack.r") # read in the p-hack function

res <- phack(initialN=30, hackrate=5, grp1M=0, grp2M=0, grp1SD=1, grp2SD=1,
   maxN=200, alpha=.05, alternative="two.sided", graph=TRUE, sims=2000)
```

## act II, demo
<img src="img/phack1.png" />

## act II, implications
- inflated $\alpha$ (p-hacking!)
- assume you tested 20 participants, and get $p = 0.057$
- unethical
- money & ressources
- just downright horrible

## act II, Bayes note
> "the rules governing when data collection stops are irrelevant to data interpretation. It
> is entirely appropriate to collect data **until a point has been proven or disproven**, or until
> the data collector runs out of time, money, or patience."

- Edwards et al. (1963)


## act III <!-- Fabian -->
- don't quantify statistical evidence; @wagenmakers2007practical
- $p = 0.04, n = 10$ is more evidence than $p = 0.04, n = 1000$
- in fact, the latter is support for $H_0$!
- are violently biased against $H_0$

## act IV <!-- Fabian -->
- violently biased against $H_0$
- underlying logic of p-values: either the null hypothesis is false, or a rare event has occured
- following example taken from Rouder et al. (in press)

## p-values: flawed reasoning
- the following is logically correct:
- (Premise): If Hypothesis $H$ is true, then event $X$ will not occur.
- (Premise): Event $X$ occured.
- (Conclusion): Hypothesis $H$ is not true.

## p-values: flawed reasoning
- this **does not** translate to probabilistic settings
- (Premise): If Hypothesis $H$ is true, then event $X$ is **unlikely**.
- (Premise): Event $X$ occured.
- (Conclusion): Hypothesis $H$ is **probably not true**.

## p-values: flawed reasoning
- take this example as demonstration:
- (Premise): If Jane is an American, then it will be unlikely that she is a U. S. Congressperson.
- (Premise): Jane is a U. S. Congressperson.
- (Conclusion): Jane is probably not an American.
- by only looking at $p(D|H_0)$, p-values are violently biased against $H_0$

## the case of Sally Clark
- both Clark's babies died, where $p(\text{baby dies}) = \frac{1}{8543}$
- thus the probability that both babies died is roughly 1 in 73 million
- since this is soo incredibly low ($p < 0.00001$ or whatnot), Clark **probably killed** her babies
- indeed, in November 1999, a jury found poor Sally guilty of double murder

## see any problem with that?

## the case of Sally Clark
> "The jury needs to weigh up two competing explanations for the babiesâ€™ deaths: SIDS or murder. The fact that two deaths by SIDS is
> quite unlikely is, taken alone, of little value. Two deaths by murder may well be even
> more unlikely. What matters is the relative likelihood of the deaths under each
> explanation, not just how unlikely they are under one explanation."

- **President of the Royal Statistical Society** (2002)

## the case of Sally Clark
- in fact, $p(\text{baby dies} | \text{sudden infant death})$ is higher than $p(\text{baby dies} | \text{murder})$
- the ratio of these two - the **likelihood ratio** - is the proper measure of statistical evidence
- statistical evidence is **always** relative; there is **no free lunch**


## act V <!-- Fabian -->
- p-value give you $p(\text{D or more extreme}|H_0)$
- what we want is $p(H|D)$, the probability that our hypothesis is true!
- there is a subtle, but important difference
- $p(\text{you are dead} | \text{shark has bitten off your head})$ is very high :(
- $p(\text{shark has bitten off your head} | \text{you are dead})$ is very low!


## Effect Sizes and Confidence Intervals: The *Frequentist* Solution?
- Effect Size
    * A measure of the strength of an effect
    * e.g., a correlation of *r* = .35, a difference between experimental conditions of Cohen's <em>d</em> = 0.30
    
- Confidence interval
    * The area within which in 95% of replications (same experiment/measures, same sample size drawn from the same population) the real parameter (effect) will be

- Example:
    * *There was a significant difference between the control group and the intervention group, p = .002, d = 0.34 [0.22, 0.44]*

## Effect Sizes and Confidence Intervals: A Solution?

<div align="center">
<img height='400' src='img/cummingcover.png'>
</div>

## Cumming, 2014
* *"We need to make substantial changes to how we conduct research"*

## Cumming, 2014
- Replicate (x)
- Adapt meta-analytical thinking (x)
- Avoid NHST (x)
- Don't trust any p-value (x)

## Cumming, 2014

<div align="center">
<img height='500' src='img/cumming_25table.png'>
</div>

## Cumming, 2014

- ES are the main research outcome (...?)
- The CI tells us the precision of a study (...?)
    * ...a much better approach than declaring the result "statistically significant" (...?)
- "Enjoy the benefits!" (...?)

## Cumming, 2014

* So, what does a SINGLE confidence interval tell me?
* Nothing

## ES and CI are NOT the solution
### Part I: Morey et al., 2014

* "For psychological science to be a healthy science, both estimation and hypothesis testing are needed."
* Estimation is necessary in pretheoretical work before clear predictions can be made, and for theory revision.
* Hypothesis testing, not estimation, is necessary for testing the quantitative predictions of theories.
* None is more informative than the other
* They answer different questions.
* Estimation alone produces a massive catalogue devoid of theoretical content
* Hypothesis testing alone may cause researchers to miss rich, meaningful structure in data.
* It is crucial for estimation and hypothesis testing to be advocated side by side

## ES and CI are NOT the solution
### Part II: Hoekstra et al., 2014

<div align="center">
<img height='400' src='img/hoekstra_CIquestions.png'>
</div>

## ES and CI are NOT the solution
### Part II: Hoekstra et al., 2014

<div align="center">
<img height='400' src='img/hoekstra_results.png'>
</div>

## ES and CI are NOT the solution
### Part III: Lee, 2014

(Lee, [here](https://webfiles.uci.edu/mdlee/Lee2014_NewStatistics.pdf))

<div align="center">
<img height='400' src='img/michaellee.png'>
</div>

## Confidence Intervals: A Solution?

CUMMING BASHING

TRANSITION:
CIs are not the solution; there is something deeply inherent about frequentist statistics that does not allow hypothesis testing the way we would like to. It is at the roots of the statistical school: The conception of probability.
As relative frequency.
Conceptualizing probability as frequency has lead to p value hypothesis testing.
We now know that:
- p-values are subjective
- p-values are biased against $H_0$
- p-values don't quantify statistical evidence
- p-values address the wrong question
- confidence intervals are equally flawed
- inference based on p-values cannot support $H_0$
- but invariances are of theoretical importance!

Now we come to the alternative.
A different conceptualization of probability that leads to different parameter estimation and hypothesis testing.
Probability as subjective belief.
Bayesian Statistics.

# The principled Bayesian

## Probability: Sum rule
- an urn holds $N$ balls: $R$ red, $B$ blue, the rest white
- uncertainty of drawing a red ball: $R / N$
- uncertainty of drawing a coloured ball: $R / N + B / N$

## Probability: Product rule
- an urn holds $N$ balls: $R$ red, $1 - R$ white, $S$ spotted, $1 - S$ plain
- given that, there are $T$ spotted *and* red balls
- uncertainty of drawing such a ball is: $T / N$
- $\frac{T}{N} = \frac{R}{N} \times \frac{T}{R} = \frac{S}{N} \times \frac{T}{S}$
- what does this mean?

## Probability: Product rule {.small}
+---------------+---------------+--------------------+--------------------+
|               |    RED        |      WHITE         |                    |
+===============+===============+====================+====================+
| SPOTTED       |     T         |                    |         S          |
+---------------+---------------+--------------------+--------------------+
| PLAIN         |               |                    |       1 - S        |
+---------------+---------------+--------------------+--------------------+
|               |     R         |      1 - R         |         N          |
+---------------+---------------+--------------------+--------------------+

## Probability: Product rule {.small}
- we want to find $P(\text{spotted}, \text{red}) = T / N$
$$
\begin{align*}
P(\text{spotted}, \text{red}) &= P(\text{spotted}|\text{red})P(\text{red}) \\[1ex]
\frac{T}{N} &= \frac{T}{R} \times \frac{R}{N} \\[3ex]
P(\text{spotted}, \text{red}) &= P(\text{red}|\text{spotted})P(\text{spotted}) \\[1ex]
\frac{T}{N} &= \frac{T}{S} \times \frac{S}{N}
\end{align*}
$$


## Implication of Sum rule
- $P(\text{red}) = P(\text{red}|\text{spotted}) + P(\text{red}|\text{plain})$
- $P(\text{red})$ is called a marginal probability
- by summing up all the conditional probabilities, we arrive at the marginal
- this will be important when we talk about the marginal likelihood in model comparison


## Summing up
- we have derived the intuitive product and sum rule of probability
- this is the core of Bayesian statistics -- I'm not kidding!
- the rest is just being smart about how to apply them


## Derivation of Bayes' rule
- it's just conditional probability:
$$
\begin{align}
P(\text{spotted}, \text{red}) &= P(\text{spotted}|\text{red})P(\text{red}) \\[1ex]
P(\text{spotted}, \text{red}) &= P(\text{red}|\text{spotted})P(\text{spotted}) \\[1ex]
\end{align}
$$

- which yields Bayes' Rule:

$$
\begin{align*}
P(\text{spotted}|\text{red})P(\text{red}) &= P(\text{red}|\text{spotted})P(\text{spotted}) \\[1ex]
P(\text{spotted}|\text{red}) &= \frac{P(\text{spotted}) \times P(\text{red}|\text{spotted})}{P(\text{red})}
\end{align*}
$$

## Bayes' rule: intuition
- what is the probability of my hypothesis, p($H$), given the data, $\textbf{y}$, I have collected?
$$
p(H|\textbf{y}) = \frac{p(H)p(\textbf{y}|H)}{p(\textbf{y})}
$$


## Some terminology
- with parameter vector $\theta$ and data $\textbf{y}$:
$$
\begin{align*}
p(\theta|\textbf{y}) &= \frac{p(\textbf{y}|\theta)p(\theta)}{p(\textbf{y})} \\[1ex]
\text{posterior} &= \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}} \\[2ex]
\text{posterior} &\sim \text{likelihood} \times \text{prior}
\end{align*}
$$

## Binomial example
- first we quantify our beliefs with a prior distribution, $p(\theta)$
- we specify our statistical model, $p(\textbf{y}|\theta)$
- then we just plug in Bayes' rule, dropping the marginal likelihood:
$$
p(\theta|\textbf{y}) \sim p(\textbf{y}|\theta)p(\theta)
$$

## Binomial example: prior I
![prior1](img/coin_prior1.png)

## Binomial example: prior II
![prior2](img/coin_prior2.png)

## Binomial example: prior III
![prior3](img/coin_prior3.png)


## Conjugacy
- a conjugate prior is a prior that when combined with the likelihood yields a posterior that is of the same
distributional family as the prior
- the parameters of conjugate priors can be interpreted as prior data

$$
\begin{align*}
\mathcal{N}(\mu_0, \sigma_0^2) &\text{ is conjugate for } \mu \text{ in } \mathcal{N}(\mu, \sigma^2) \\
\mathcal{IG}(\alpha, \beta) &\text{ is conjugate for } \sigma^2 \text{ in } \mathcal{N}(\mu, \sigma^2) \\
\mathcal{Beta}(\alpha, \beta) &\text{ is conjugate to } \mathcal{Bin}(n, p) \\
\mathcal{Beta}(\alpha, \beta) &\text{ is conjugate to } \mathcal{NegBin}(n, p) \\
\mathcal{Dir}(\omega) &\text{ is conjugate to } \mathcal{Mult}(\theta) \\
\mathcal{G}(\alpha, \beta) &\text{ is conjugate to } \mathcal{Pois}(n, \lambda) \\
\mathcal{G}(\alpha, \beta) &\text{ is conjugate to } \mathcal{Exp}(n, \lambda) \\
\mathcal{Pareto}(\alpha, \beta) &\text{ is conjugate to } \mathcal{Unif}(a, b) \\
\end{align*}
$$

## Binomial example: likelihood
- instead of $p(\textbf{y}|\theta)$ we write $\mathcal{L}(\theta; \textbf{y})$
- for the binomial case, we use:
$$
\mathcal{L}(\theta; k, N) = \theta^k \times (1 - \theta)^{N - k}
$$

where $k$ is the number of successes and $N$ the number of data points

## Binomial example: likelihood
- likelihoods are relative, and are *not* probabilities!
- they indicate how likely the data is, given certain parameter values
- assume $k = 2$, $N = 4$:

$$
\begin{align*}
\mathcal{L}(\theta = .5; k = 2, N = 4) = .5^2 \times (1 - .5)^{4 - 2} = .375 \\[1ex]
\mathcal{L}(\theta = .9; k = 2, N = 4) = .9^2 \times (1 - .9)^{4 - 2} = .049
\end{align*}
$$

## Binomial example: likelihood
- likelihoods quantify statistical evidence
- $\theta = .5$ is $.375 / .049 = 7.65$ times more likely than $\theta = .9$ 
- using this method, we can only compare *point* hypotheses
- later we will see how Bayes factors relax this restriction


## Binomial example: posterior
- the posterior is simply a combination of the prior and the likelihood
- in binomial settings, we a have simple updating rule:
$$
\begin{align*}
p(\theta) &\sim \mathcal{Beta}(a, b) \\[1ex]
p(\theta|\textbf{y}) &\sim \mathcal{Beta}(a + k, b + N - k)
\end{align*}
$$


## Binomial example: shiny
- Visualisations from a great blog post by [Alexander Etz](http://alexanderetz.com/2015/08/09/understanding-bayes-visualization-of-bf/)
- explain:
    * Bayesian updating
    * Cromwell's Rule
    * flat priors and maximum likelihood


## Interim conclusion II
- Bayesian statistics follows from probability
- it thus inherits favourable properties that classical statistics lacks
- **rationality**: if one violates Bayesian reasoning, one can systematically loose money
- **coherence**: the order of updating one's belief does not matter
- **intuitive**: it allows us to say how probable our hypothesis is after the experiment


## Model comparison
- how strongly can we believe in our hypothesis, given the collected data?
- that is, we want $p(H|\textbf{y})$
- a hypothesis can be instantiated in a model
- $H_0: \delta = 0$ corresponds to a model where the parameter $\delta$ is fixed to 0
- $H_1: \delta \neq 0$ corresponds to a model where the parameter $\delta$ is free to vary

## Bayes factor: Derivation
- assume two models, $M_0$ and $M_1$, that instantiate $H_0$ and $H_1$, respectively
- after we have collected data, $p(\textbf{y})$, which model should we prefer?
- the probability of each model is computed using Bayes' rule:

$$
\begin{align}
p(M_0|\textbf{y}) &= \frac{p(\textbf{y}|M_0)p(M_0)}{p(\textbf{y})} \\[1ex]
p(M_1|\textbf{y}) &= \frac{p(\textbf{y}|M_1)p(M_1)}{p(\textbf{y})}
\end{align}
$$

## Bayes factor: Derivation

$$
\begin{align}
\frac{p(M_0|\textbf{y})}{p(M_1|\textbf{y})} &= \frac{\frac{p(\textbf{y}|M_0)p(M_0)}{p(\textbf{y})}} {\frac{p(\textbf{y}|M_1)p(M_1)}{p(\textbf{y})}} \\[2ex]
\frac{p(M_0|\textbf{y})}{p(M_1|\textbf{y})} &= \frac{p(M_0)}{p(M_1)}\frac{p(\textbf{y}|M_0)}{p(\textbf{y}|M_1)} \\[2ex]
\text{posterior odds} &= \text{prior odds} \times \text{Bayes factor}
\end{align}
$$

## Bayes factor: Intuition
- the Bayes factor is an updating factor
- it tells us how to update our prior beliefs about the hypotheses, given the data
- note that there are two priors: over models $M$, and over parameters $\theta$
- the term $p(\textbf{y}|M_0)$ is the marginal likelihood under model $M_0$
- in other words, it is the probability of the data under model $M_0$
- it quantifies how well this model predicts the data

## Bayes factor: Marginal likelihood
- in parameter estimation, we have avoided this term
- now that we compare models, we cannot do so!
- often it is a high-dimensional integral, which is difficult to compute
- for $M_0$, it is:
$$
p(\textbf{y}|M_0) = \int p(\textbf{y}|\theta, M_0)p(\theta|M_0)\mathrm{d}\theta
$$

- this simply follows from the sum rule discussed before
- it is a weighted average of the likelihood with respect to the prior


## Bayes factor: Interpretation
- the Bayes factor is a generalization of the likelihood ratio
- recall before, we compared $H_0: \theta = .5$ against $H_1: \theta = .9$
- Bayes factors let us test composite hypothesis, like $H_0: \theta \neq 0$
- for this, we have to specify a prior distribution over $\theta$

## Bayes factor: shiny demo
- explain:
    * Bayes factor interpretation
    * Savage-Dickey density ratio
    * parameter estimation versus hypothesis testing
    * Bayes factor always interpretable -- regardless of sample size

## Bayes factor: Savage-Dickey density ratio
- a neat mathematical trick to avoid computing the marginal likelihood:

$$
BF_{01} = \frac{p(\delta = 0|H_1, \textbf{y})}{p(\delta = 0|H_1)}
$$


## Bayes factor: Jeffreys-Lindley's paradox
- uninformative priors should not be used
- they lead to unbounded support for $H_0$
- BayesFactor, JASP use default priors


## Interim conclusion III
- Bayes factor quantifies statistical evidence
- allows us to state evidence for $H_0$
- no issue with sequential testing
- does not depend on the sample size
- if the data are uninformative, the Bayes factor will tell you exactly that


## Principled conclusion
- again, the sum and product rule are the core of Bayesian statistics
- "Bayesian" is somewhat of a misnomer
- everything follows from the sum and product rule
- the rest is just being smart on how to apply them


# The practical Bayesian
## Setup
- do hats boost creativity?

```{r}
set.seed(1774)

hat <- rnorm(50, 60, 10)
nohat <- rnorm(50, 50, 10)
dat <- data.frame(score = c(nohat, hat), hat = rep(0:1, each = 50))
```

## Setup
![hats_boxplot](img/creativity_hats.png)

## Bayesian approach
- specify a joint distribution over parameters and data, $p(\textbf{y}, \mu, \sigma^2)$, thus:

$$
\begin{align*}
p(\mu, \sigma^2|\textbf{y}) &= \frac{p(\textbf{y}|\mu, \sigma^2)p(\mu, \sigma^2)}{p(\textbf{y})} \\[1ex]
p(\mu, \sigma^2|\textbf{y}) &\sim p(\textbf{y}|\mu, \sigma^2)p(\mu, \sigma^2)
\end{align*}
$$


## Likelihood -- our statistical model
- we assume that the data are normally distributed; for a single datum, thus:
$$
\begin{equation*}
\mathcal{L}(\mu, \sigma^2; y) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp(\frac{-(y - \mu)^2}{2\sigma^2})
\end{equation*}
$$

- assuming *exchangeability*, we can write:

$$
\begin{equation*}
\mathcal{L}(\mu, \sigma^2; \textbf{y}) = \prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp(\frac{-(y_i - \mu)^2}{2\sigma^2})
\end{equation*}
$$


## Likelihood -- what is it?
- likelihood denotes the probability of the data, given certain fixed parameter values

$$
\begin{align*}
\mathcal{L}(\mu = 40, \theta = 15; y = 50) &= .0213 \\
\mathcal{L}(\mu = 50, \theta = 15; y = 50) &= .0266
\end{align*}
$$


## Prior
- In Bayesian inference, we need to formalize our belief before seeing the data
- doing this over $\mu$ and $\sigma^2$ simultanously is difficult
- thus we make the simplifying assumption that the priors are independent
- $p(\mu, \sigma^2) = p(\mu)p(\sigma^2)$
- because we are simply estimating parameters, we do not need to specify informative priors

## Prior specification

$$
\begin{align*}
p(\mu) &\sim \mathcal{N}(\mu_0, \omega_0^2) \\
p(\sigma^2) &\sim \mathcal{IG}(v_0, \frac{v_0 \sigma_0^2}{2})
\end{align*}
$$

- we specify a normal over $\mu$, and an inverse gamma over $\sigma^2$
- this prior specification is *semi-conjugate*:

$$
\begin{align*}
p(\mu|\textbf{y}, \sigma^2) &\sim \mathcal{N}(\mu, \omega^2) \\
p(\sigma^2|\textbf{y}, \mu) &\sim \mathcal{IG}(v_1, S)
\end{align*}
$$

where $\omega^2$ depends on $\sigma^2$ and $S$ depends on $\mu$

## Posterior distribution
- we don't get marginal posterior distributions, $p(\mu|\textbf{y})$ and $p(\sigma^2|\textbf{y})$ when using independent priors
- but it is those that we want for drawing inferences!
- use **Markov chain Monte Carlo** methods (in this case: *Gibbs Sampling*)
- let's first discuss Monte Carlo on a simple example

## distribution of reaction times
- reaction times are not (log)normally distributed
- instead, an Ex-Gaussian distribution fits best
- it is the sum of an exponential and a Gaussian
$$
x | \eta \sim \mathcal{N}(\mu + \eta, \sigma^2) \\[1ex]
\eta \sim \mathcal{Exp}(\nu)
$$

## distribution of reaction times
- but we do not want $x | \eta$ (conditional probability)
- we want $x$ (marginal probability)
- analytically, one would apply the sum rule and "integrate out" $\nu$
- however, that is not tractable -- so instead, we use Monte Carlo methods
- this are computer based methods to solve hard problems

## Monte Carlo principle
- **anything we want to know about a random variable $\theta$ can be learned by sampling
many times from f($\theta$), the density of $\theta$**
- in R, we can sample randomly from different probability distributions
- for example, "rnorm", "rbeta", "rgamma", "rexp"

## Monte Carlo integration
- we sample from $\eta$ using "rexp" and use this value for $x | \eta$ in "rnorm"

## Monte Carlo integration
```{r}
exgauss <- function(mu, sigma, rate, times = 1000) {
  res <- rep(NA, times)
  
  for (i in 1:times) {
    nu <- rexp(1, rate)
    res[i] <- rnorm(1, mu + nu, sigma)
  }
  
  res
}
```

## Monte Carlo integration
![exgauss](img/exgauss.png)

## Monte Carlo integration
- is quite a neat technique!
- let's generalize it to Gibbs Sampling
- in our creativity example, both distributions are conditional on each other
- in a sense, now we have to do monte carlo integration for both of them
- we do this iteratively, in each step drawing from both distributions, conditional on the other value

## Gibbs Sampling
```{r}
gibbs <- function(mu0, w0, v0, sigma0, y, n.iter = 8000, burnin = n.iter / 4) {
  n <- length(y)
  
  v1 <- v0 + n
  mu_post <- rep(mean(y), n.iter)
  var_post <- rep(var(y), n.iter)
  
  for (i in 2:n.iter) {
    mu <- mu_post[i-1]
    sigma2 <- var_post[i-1]
    w1 <- 1 / (n / sigma2 + 1 / w0^2) # condition on current variance
    mu1 <- ((n / sigma2) * mean(y) + (1 / w0^2) * mu0) / (n / sigma2 + 1 / w0^2)
    mu_post[i] <- rnorm(1, mu1, sqrt(w1))
    S <- sum((y - mu)^2) # condition on current mean
    var_post[i] <- 1 / rgamma(1, v1 / 2, S / 2)
  }
  cbind(mu_post, var_post)[-(1:burnin), ]
}
```

## Gibbs Sampling
- why is this called a Markov chain Monte Carlo technique?
- Markov chain because the values are dependent on each other
- we don't want this, so we "burn-in" the first few samples
- we can also do thinning -- only keep every second draw
- it is important to check convergence

## Posterior distributions
- for parameter estimation we can specify uninformative priors:
$$
\begin{align}
\omega_0^2 = 1000 \\
\nu_0 = .0001 \\
\sigma_0^2 = .0001
\end{align}
$$

## Posterior distributions
![creativity_posterior](img/creativity_posterior.png)

## Is our model adequate?
- the Bayes factor is relative
- just because a model is favoured very strongly against another, doesn't make it any good
- we have to check for the model's adequacy -- against the real world
- in a Bayesian setting, this is done using *posterior predictive checks*
- we simulate data from our model and check if it gravely misrepresents the observed data

## Posterior predictive checks
- more formally, we compute:
$$
p(\textbf{y}_{rep}|\textbf{y}) = \int p(\textbf{y}_{rep},|\theta)p(\theta|\textbf{y})\mathrm{d}\theta
$$
- we can then use plots and visualisations to assess possible discrepancies

## Posterior predictive checks: no-hat condition
![creativity_nohat](img/creativity_nohat_ppc.png)

## Posterior predictive checks: hat condition
![creativity_hat](img/creativity_hat_ppc.png)

## Posterior predictive checks
- alright -- but we might want to quantify the discrepancy
- and test whether there is *significant* deviation
- yes, I just used the word *significant*
- for each posterior predictive sample, we can apply a function *T* which returns some
value of interest (test statistic)
- this yields a distribution of those values
- we can then apply the function to our observed data, and see if the outcome is extreme
w.r.t. to the distribution

## Posterior predictive *p* values
- formally, we compute:
$$
p = p(T(y_{rep}, \theta)) \geq T(\textbf{y}, \theta|\textbf{y}))
$$

## Posterior predictive *p* values
- for the hat condition it looks like there might be outliers
- a simple (naive?) approach would be to use *min* as the test statistic

## Posterior predictive *p* values
![creativity_min](img/creativity_ppc_min.png)

## Effect size
![creativity_effect](img/creativity_effect.png)

## Effect size
- in this case, it's obvious that hats make a substantial difference
- these data pass the *interocular traumatic test*
- they are so obvious -- it hits you right between the eyes!
- in the real world, however, things are rarely that clear:
- "... the enthusiast's interocular trauma may be the skeptic's random error. A little arithmetic to verify the extent of the trauma can yield great peace of mind for little cost." [@edwards1963bayesian, p. 217]

# Software for Bayesian data analysis

## JASP: Example
- developed by a research group around Jonathon Love in Amsterdam
- provides a free, open-source, modern alternative to SPSS
- does frequentist as well as Bayesian inference
- computes Bayes factors -- providing us with a great peace of mind

## JASP: Example
- in replication research, being able to support the null hypothesis is crucial
- Topolski and Sparenberk (2012) found that counter clockwise movements lead to an orientation towards the future and novelty
- Wagenmakers et al. (2015) directly replicated this research

## JASP: Example

*Turning the hands of time - with kitchen rolls!*

<img src="img/turning_hands.png" height="500" width="700"/>

## JASP: Example
- you can find the pre-registration form of this research [here](https://osf.io/p3isc/), see the section **sampling plan**
- they failed to replicate the effect; the data were about 10 times more likely under $H_0$ then under $H_1$
- descriptives point in the other direction than originally observed (higher openness when counter clock wise)

## JASP: Example

*Bathing habits - compensating loneliness wit heat!*

- lonely people compensate the lack of social warmth by taking warmer showers and baths [@bargh2012bath]
- ummmmmmm, really?
- @donnellan2014association tried to replicate this in 9 experiments with over 3000 participants
- @wagenmakers2015absence re-analysed the data using Bayesian inference

## JASP: Example
- *p* values are uniformly distributed under $H_0$
- Difference: $p > 0.05$, the data are uninformative
  vs. $p > 0.05$, the data are informative and support $H_0$
- Frequentist statistics:
  We don't know which one is the case
- Bayesfactor:
  We know which one is the case!
  -> Quantifies support for $H_0$ 


## JASP: Practice

## Bayesian SEM

Structural Equation Modeling

> - Analysis of (variance) **covariance** (mean) structures

> - Emphasis on *latent variables*

## Bayesian SEM

Mean

## Bayesian SEM

## Dataset

- For whole talk

- executive functions, intelligence, creativity, insight

- Focus on creativity-question (overlaps with bayes-posts)

- *n* = 230 university students


# Practice: Fitting and Evaluating typical models the Bayesian way

JASP
NO BayesFactor
Mplus


## JASP

15-20 minutes

t-test (group comparison; e.g., gender)
(RM)ANOVA (repeated measures)
mult reg

Practicing interpretation of CIs, BFs, prior choice

## Why now Bayes?

- Similarity

# Typical Models the Elegant Way: Bayesian SEM {.flexbox .vcenter}

5 -7 minutes

Models from the WiPs

From simple (t-test) to complex (full SEM)

H0 quantifizieren, sequentielles sampling

-> Falls 

## Bayesian SEM: t-test

> - Relax :)

## Bayesian SEM: (Multiple) Regression

## Bayesian SEM: RM t-test

## Bayesian SEM: RMANOVA

## Bayesian SEM: MANOVA

## Bayesian SEM: RMMANOVA -> Cross-lagged model

## Bayesian SEM: RMMANOVA -> Cross-lagged model -> Change score model

## Bayesian SEM: CFA

## Bayesian SEM: Group Comparisons

> - Mean comparisons

> - Multigroup CFA

## Bayesian SEM: Super huge model

## Reporting Bayesian Analysis

- **Software for Bayesian data analysis** <!-- Peter, 40 min [110] -->
    * 5.1 JASP: Examples
    * 5.2 JASP: Practice
    * 5.2 Demo: Bayesian SEM

- **Discussion** <!-- Peter, 10 min [120] -->

    * Outlook
    * Questions


## References
